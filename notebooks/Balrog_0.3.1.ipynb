{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Balrog_0.3.1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEszqvakZN9C",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMJQa6JAXDf_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b9a4cc0-0872-4f69-bad7-9ca6a4bb74c2"
      },
      "source": [
        "# @title Install dependencies\n",
        "!pip install biopython\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install scipy\n",
        "!pip install google.colab\n",
        "print(\"\\nDone\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /home/animeshs/miniconda3/lib/python3.7/site-packages (1.78)\r\n",
            "Requirement already satisfied: numpy in /home/animeshs/miniconda3/lib/python3.7/site-packages (from biopython) (1.19.0)\n",
            "Requirement already satisfied: torch in /home/animeshs/miniconda3/lib/python3.7/site-packages (1.6.0)\n",
            "Requirement already satisfied: future in /home/animeshs/miniconda3/lib/python3.7/site-packages (from torch) (0.18.2)\n",
            "Requirement already satisfied: numpy in /home/animeshs/miniconda3/lib/python3.7/site-packages (from torch) (1.19.0)\n",
            "Requirement already satisfied: pandas in /home/animeshs/miniconda3/lib/python3.7/site-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas) (1.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas) (2020.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /home/animeshs/miniconda3/lib/python3.7/site-packages (4.46.0)\n",
            "Requirement already satisfied: scipy in /home/animeshs/miniconda3/lib/python3.7/site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from scipy) (1.19.0)\n",
            "Processing /home/animeshs/.cache/pip/wheels/f6/3b/58/f34ea9045a7c69bd5634978bf25ac60277e90997d9e6e74192/google_colab-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython~=5.5.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (5.5.0)\n",
            "Requirement already satisfied: six~=1.12.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (1.12.0)\n",
            "Requirement already satisfied: google-auth~=1.4.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (1.4.2)\n",
            "Requirement already satisfied: ipykernel~=4.6.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (4.6.1)\n",
            "Requirement already satisfied: notebook~=5.2.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (5.2.2)\n",
            "Requirement already satisfied: requests~=2.21.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (2.21.0)\n",
            "Requirement already satisfied: tornado~=4.5.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (4.5.3)\n",
            "Requirement already satisfied: portpicker~=1.2.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (1.2.0)\n",
            "Requirement already satisfied: pandas~=0.24.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google.colab) (0.24.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (46.4.0.post20200518)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (5.0.4)\n",
            "Requirement already satisfied: pygments in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (2.6.1)\n",
            "Requirement already satisfied: decorator in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (4.1.1)\n",
            "Requirement already satisfied: jupyter-client in /home/animeshs/miniconda3/lib/python3.7/site-packages (from ipykernel~=4.6.0->google.colab) (6.1.7)\n",
            "Requirement already satisfied: jupyter-core in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (4.6.3)\n",
            "Requirement already satisfied: nbformat in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (5.0.7)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (0.8.3)\n",
            "Requirement already satisfied: ipython-genutils in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: jinja2 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /home/animeshs/miniconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (5.6.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (2020.4.5.1)\n",
            "Requirement already satisfied: pytz>=2011k in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (2020.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (1.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (2.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython~=5.5.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /home/animeshs/miniconda3/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from rsa>=3.1.4->google-auth~=1.4.0->google.colab) (0.4.8)\n",
            "Requirement already satisfied: pyzmq>=13 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from jupyter-client->ipykernel~=4.6.0->google.colab) (19.0.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbformat->notebook~=5.2.0->google.colab) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from jinja2->notebook~=5.2.0->google.colab) (1.1.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n",
            "Requirement already satisfied: defusedxml in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.2)\n",
            "Requirement already satisfied: testpath in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.4.4)\n",
            "Requirement already satisfied: bleach in /home/animeshs/miniconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/animeshs/miniconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (1.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (20.2.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (0.17.1)\n",
            "Requirement already satisfied: webencodings in /home/animeshs/miniconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
            "Requirement already satisfied: packaging in /home/animeshs/miniconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (20.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/animeshs/miniconda3/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook~=5.2.0->google.colab) (2.4.7)\n",
            "Installing collected packages: google.colab\n",
            "Successfully installed google.colab\n",
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe1PHlcjeHS8",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34fa4ed3-5809-4d68-80ce-9ade90b881ab"
      },
      "source": [
        "# @title Import python packages\n",
        "import os\n",
        "import sys\n",
        "import gzip\n",
        "import copy\n",
        "import time\n",
        "import pandas\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from Bio import SeqIO\n",
        "from scipy.special import expit\n",
        "from scipy.special import logit\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "# # import networkx as nx\n",
        "# from scipy.sparse import coo_matrix\n",
        "# from scipy.sparse import csr_matrix\n",
        "# from scipy.sparse.csgraph import bellman_ford\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usWmfqJNhM2X",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56cbd0e4-dd4c-4723-8074-32eea77fcf71"
      },
      "source": [
        "# @title Set parameters (double click top of cell to change defaults)\n",
        "\"\"\" Print what the program is doing.\"\"\"\n",
        "verbose = True\n",
        "\n",
        "\"\"\" Maximum ORF (open reading frame) overlap length in nucleotides.\"\"\"\n",
        "max_gene_overlap = 60\n",
        "\n",
        "\"\"\" Minimum ORF length in nucleotides.\"\"\"\n",
        "min_orf_length = 60\n",
        "\n",
        "\"\"\" Use kmer prefilter to increase gene sensitivity. \n",
        "May not play nice with very high GC genomes.\"\"\"\n",
        "protein_kmer_filter = True\n",
        "\n",
        "\"\"\" Use mmseqs2 and a gene score cutoff to remove most false positive predictions.\"\"\"\n",
        "mmseqs2_gene_filter = True\n",
        "\n",
        "\"\"\" Nucleotide to amino acid translation table. 11 for most bacteria/archaea.\n",
        "4 for Mycoplasma/Spiroplasma.\"\"\"\n",
        "translation_table = 11\n",
        "# translation_table = 4\n",
        "\n",
        "\"\"\" Maximum number of forward connections in the directed acyclic graph used to\n",
        "find a set of coherent genes in each genome.\n",
        "Higher values will slow execution time and increase memory usage,\n",
        "but may slightly increase performace.\n",
        "Recommended range ~30-50\"\"\"\n",
        "max_forward_connections = 50\n",
        "\n",
        "\"\"\" Batch size for the temporal convolutional network used to score genes.\n",
        "Small batches and big batches slow down the model. Very big batches may crash the \n",
        "GPU. \"\"\"\n",
        "gene_batch_size = 200\n",
        "TIS_batch_size = 1000\n",
        "\n",
        "\"\"\" All following are internal parameters. Change at your own risk.\"\"\"\n",
        "weight_gene_prob = 0.9746869839852076 \n",
        "weight_TIS_prob = 0.25380288790532707 \n",
        "score_threshold = 0.47256101519707244\n",
        "weight_ATG = 0.84249804151264 \n",
        "weight_GTG = 0.7083689705744909\n",
        "weight_TTG = 0.7512400826652517 \n",
        "unidirectional_penalty_per_base = 3.895921717182765 # 3' 5' overlap\n",
        "convergent_penalty_per_base = 4.603432608883688 # 3' 3' overlap\n",
        "divergent_penalty_per_base = 3.3830814940689975 # 5' 5' overlap\n",
        "k_seengene = 10\n",
        "multimer_threshold = 2\n",
        "model_dir = \"/home/animeshs/Balrog\"\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idOvBwhe0NtP",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a1db9b44-98a1-40d8-c6a1-b496daf8bb44"
      },
      "source": [
        "# @title Load pre-trained gene and translation initiation site models\n",
        "\n",
        "\"\"\" if you're interested in the inner workings of the \n",
        "temporal convolutional network, see hubconf.py in the Github repo below.\"\"\"\n",
        "\n",
        "# repo = \"/home/animeshs/Balrog\"\n",
        "repo = \"salzberg-lab/Balrog:develop\"\n",
        "\n",
        "torch.hub.set_dir(model_dir)\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(\"GPU detected...\")\n",
        "    model = torch.hub.load(repo, \"geneTCN\", force_reload=True).cuda()\n",
        "    model_tis = torch.hub.load(repo, \"tisTCN\", force_reload=False).cuda()\n",
        "    time.sleep(0.5)\n",
        "    print(\"\\nDone\")\n",
        "else:\n",
        "    print(\"No GPU detected, using CPU...\")\n",
        "    model = torch.hub.load(repo, \"geneTCN\", force_reload=True)\n",
        "    model_tis = torch.hub.load(repo, \"tisTCN\", force_reload=False)\n",
        "    time.sleep(0.5)\n",
        "    print(\"\\nDone\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU detected, using CPU...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/salzberg-lab/Balrog/archive/develop.zip\" to /home/animeshs/Balrog/develop.zip\n",
            "Using cache found in /home/animeshs/Balrog/salzberg-lab_Balrog_develop\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQMPFNbdk8vv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "64eeb56c-15a6-4333-c190-a10ee26277ca"
      },
      "source": [
        "# @title Prepare protein kmer filter\n",
        "if protein_kmer_filter:\n",
        "    # decompress kmer filter\n",
        "    seengene_dir = \"/home/animeshs/seegene/\"\n",
        "    !mkdir {seengene_dir}\n",
        "    %cd {seengene_dir}\n",
        "    !tar -xvzf /home/animeshs/Balrog/kmer_filter/genexa_10mer_thresh2_minusARF_all.tar.gz\n",
        "    # !tar -xvzf /home/salzberg-lab_Balrog_develop/kmer_filter/genexa_10mer_thresh2_minusARF_all.tar.gz\n",
        "    genexa_kmer_path = os.path.join(seengene_dir, \"10mer_thresh2_minusARF_all.pkl\")\n",
        "\n",
        "    # load kmer filter\n",
        "    with open(genexa_kmer_path, \"rb\") as f:\n",
        "        aa_kmer_set = pickle.load(f)\n",
        "\n",
        "print(\"\\nDone\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/animeshs/seegene/’: File exists\r\n",
            "/home/animeshs/seegene\n",
            "10mer_thresh2_minusARF_all.pkl\n",
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3jzwdcGp7XN",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eea67e58-0213-43da-ef17-0743600b173a"
      },
      "source": [
        "# @title Prepare mmseqs2\n",
        "\n",
        "if mmseqs2_gene_filter:\n",
        "    ![ $(uname -m) = \"x86_64\" ] && echo \"64bit: Yes\" || echo \"64bit: No\"\n",
        "    !grep -q sse4_1 /proc/cpuinfo && echo \"SSE4.1: Yes\" || echo \"SSE4.1: No\"\n",
        "    !grep -q avx2 /proc/cpuinfo && echo \"AVX2: Yes\" || echo \"AVX2: No\"\n",
        "\n",
        "    # install\n",
        "    !mkdir /home/animeshs/content/mmseqs2\n",
        "    %cd /home/animeshs/content/mmseqs2\n",
        "    !wget https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz\n",
        "    !tar xvzf mmseqs-linux-avx2.tar.gz\n",
        "\n",
        "    # decompress fasta\n",
        "    !mkdir /home/animeshs/content/mmseqs2/genexa\n",
        "    %cd /home/animeshs/content/mmseqs2/genexa\n",
        "    !!tar -xvzf /home/salzberg-lab_Balrog_master/protein_filter/genexa_genes.tar.gz\n",
        "    # !!tar -xvzf /home/salzberg-lab_Balrog_develop/protein_filter/genexa_genes.tar.gz\n",
        "    genexa_fasta_path = \"/home/animeshs/content/mmseqs2/genexa/genexa_genes.fasta\"\n",
        "\n",
        "    # create DB\n",
        "    genexa_DB_path = \"/home/animeshs/content/mmseqs2/genexa/genexaDB\"\n",
        "    !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs createdb {genexa_fasta_path} {genexa_DB_path}\n",
        "\n",
        "    # build mmseqs index\n",
        "    !mkdir /home/animeshs/content/mmseqs2/tmp\n",
        "    !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs createindex {genexa_DB_path} /home/animeshs/content/mmseqs2/tmp\n",
        "\n",
        "    # download swissprot DB\n",
        "    !mkdir /home/animeshs/content/mmseqs2/swissprot\n",
        "    !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs databases UniProtKB/Swiss-Prot /home/animeshs/content/mmseqs2/swissprot/swissprotDB /home/animeshs/content/mmseqs2/tmp\n",
        "    swissprot_DB_path = \"/home/animeshs/content/mmseqs2/swissprot/swissprotDB\"\n",
        "    \n",
        "print(\"\\nDone\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64bit: Yes\n",
            "SSE4.1: Yes\n",
            "AVX2: Yes\n",
            "mkdir: cannot create directory ‘/home/animeshs/content/mmseqs2’: File exists\n",
            "/home/animeshs/content/mmseqs2\n",
            "--2020-09-09 13:06:33--  https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz\n",
            "Resolving mmseqs.com (mmseqs.com)... 141.5.100.26\n",
            "Connecting to mmseqs.com (mmseqs.com)|141.5.100.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30969306 (30M) [application/octet-stream]\n",
            "Saving to: ‘mmseqs-linux-avx2.tar.gz.3’\n",
            "\n",
            "mmseqs-linux-avx2.t 100%[===================>]  29.53M  11.2MB/s    in 2.6s    \n",
            "\n",
            "2020-09-09 13:06:36 (11.2 MB/s) - ‘mmseqs-linux-avx2.tar.gz.3’ saved [30969306/30969306]\n",
            "\n",
            "mmseqs/\n",
            "mmseqs/util/\n",
            "mmseqs/util/bash-completion.sh\n",
            "mmseqs/LICENCE.md\n",
            "mmseqs/README.md\n",
            "mmseqs/userguide.pdf\n",
            "mmseqs/examples/\n",
            "mmseqs/examples/DB.fasta\n",
            "mmseqs/examples/QUERY.fasta\n",
            "mmseqs/matrices/\n",
            "mmseqs/matrices/PAM80.out\n",
            "mmseqs/matrices/PAM120.out\n",
            "mmseqs/matrices/PAM90.out\n",
            "mmseqs/matrices/blosum75.out\n",
            "mmseqs/matrices/PAM110.out\n",
            "mmseqs/matrices/blosum50.out\n",
            "mmseqs/matrices/blosum85.out\n",
            "mmseqs/matrices/blosum70.out\n",
            "mmseqs/matrices/PAM170.out\n",
            "mmseqs/matrices/VTML10.out\n",
            "mmseqs/matrices/PAM160.out\n",
            "mmseqs/matrices/blosum95.out\n",
            "mmseqs/matrices/PAM150.out\n",
            "mmseqs/matrices/PAM180.out\n",
            "mmseqs/matrices/blosum35.out\n",
            "mmseqs/matrices/blosum55.out\n",
            "mmseqs/matrices/PAM70.out\n",
            "mmseqs/matrices/PAM100.out\n",
            "mmseqs/matrices/blosum40.out\n",
            "mmseqs/matrices/PAM60.out\n",
            "mmseqs/matrices/PAM190.out\n",
            "mmseqs/matrices/PAM130.out\n",
            "mmseqs/matrices/PAM50.out\n",
            "mmseqs/matrices/blosum62.out\n",
            "mmseqs/matrices/VTML20.out\n",
            "mmseqs/matrices/blosum45.out\n",
            "mmseqs/matrices/blosum30.out\n",
            "mmseqs/matrices/nucleotide.out\n",
            "mmseqs/matrices/PAM10.out\n",
            "mmseqs/matrices/blosum80.out\n",
            "mmseqs/matrices/blosum60.out\n",
            "mmseqs/matrices/PAM40.out\n",
            "mmseqs/matrices/blosum100.out\n",
            "mmseqs/matrices/VTML160.out\n",
            "mmseqs/matrices/PAM30.out\n",
            "mmseqs/matrices/blosum90.out\n",
            "mmseqs/matrices/VTML80.out\n",
            "mmseqs/matrices/PAM20.out\n",
            "mmseqs/matrices/VTML40.out\n",
            "mmseqs/matrices/blosum65.out\n",
            "mmseqs/matrices/PAM140.out\n",
            "mmseqs/matrices/VTML120.out\n",
            "mmseqs/bin/\n",
            "mmseqs/bin/mmseqs\n",
            "mkdir: cannot create directory ‘/home/animeshs/content/mmseqs2/genexa’: File exists\n",
            "/home/animeshs/content/mmseqs2/genexa\n",
            "createdb /home/animeshs/content/mmseqs2/genexa/genexa_genes.fasta /home/animeshs/content/mmseqs2/genexa/genexaDB \n",
            "\n",
            "MMseqs Version:       \tcc7d7da30ec779d6a2e886438f8295f59e2192f1\n",
            "Database type         \t0\n",
            "Shuffle input database\ttrue\n",
            "Createdb mode         \t0\n",
            "Write lookup file     \t1\n",
            "Offset of numeric ids \t0\n",
            "Compressed            \t0\n",
            "Verbosity             \t3\n",
            "\n",
            "Converting sequences\n",
            "[468338] 1s 143ms\n",
            "Time for merging to genexaDB_h: 0h 0m 0s 163ms\n",
            "Time for merging to genexaDB: 0h 0m 0s 416ms\n",
            "Database type: Aminoacid\n",
            "Time for processing: 0h 0m 2s 455ms\n",
            "mkdir: cannot create directory ‘/home/animeshs/content/mmseqs2/tmp’: File exists\n",
            "createindex /home/animeshs/content/mmseqs2/genexa/genexaDB /home/animeshs/content/mmseqs2/tmp \n",
            "\n",
            "MMseqs Version:          \tcc7d7da30ec779d6a2e886438f8295f59e2192f1\n",
            "Seed substitution matrix \tnucl:nucleotide.out,aa:VTML80.out\n",
            "k-mer length             \t0\n",
            "Alphabet size            \tnucl:5,aa:21\n",
            "Compositional bias       \t1\n",
            "Max sequence length      \t65535\n",
            "Max results per query    \t300\n",
            "Mask residues            \t1\n",
            "Mask lower case residues \t0\n",
            "Spaced k-mers            \t1\n",
            "Spaced k-mer pattern     \t\n",
            "Sensitivity              \t7.5\n",
            "k-score                  \t0\n",
            "Check compatible         \t0\n",
            "Search type              \t0\n",
            "Split database           \t0\n",
            "Split memory limit       \t0\n",
            "Verbosity                \t3\n",
            "Threads                  \t24\n",
            "Min codons in orf        \t30\n",
            "Max codons in length     \t32734\n",
            "Max orf gaps             \t2147483647\n",
            "Contig start mode        \t2\n",
            "Contig end mode          \t2\n",
            "Orf start mode           \t1\n",
            "Forward frames           \t1,2,3\n",
            "Reverse frames           \t1,2,3\n",
            "Translation table        \t1\n",
            "Translate orf            \t0\n",
            "Use all table starts     \tfalse\n",
            "Offset of numeric ids    \t0\n",
            "Create lookup            \t0\n",
            "Compressed               \t0\n",
            "Add orf stop             \tfalse\n",
            "Overlap between sequences\t0\n",
            "Sequence split mode      \t1\n",
            "Strand selection         \t1\n",
            "Remove temporary files   \tfalse\n",
            "\n",
            "createindex /home/animeshs/content/mmseqs2/genexa/genexaDB /home/animeshs/content/mmseqs2/tmp \n",
            "\n",
            "MMseqs Version:          \tcc7d7da30ec779d6a2e886438f8295f59e2192f1\n",
            "Seed substitution matrix \tnucl:nucleotide.out,aa:VTML80.out\n",
            "k-mer length             \t0\n",
            "Alphabet size            \tnucl:5,aa:21\n",
            "Compositional bias       \t1\n",
            "Max sequence length      \t65535\n",
            "Max results per query    \t300\n",
            "Mask residues            \t1\n",
            "Mask lower case residues \t0\n",
            "Spaced k-mers            \t1\n",
            "Spaced k-mer pattern     \t\n",
            "Sensitivity              \t7.5\n",
            "k-score                  \t0\n",
            "Check compatible         \t0\n",
            "Search type              \t0\n",
            "Split database           \t0\n",
            "Split memory limit       \t0\n",
            "Verbosity                \t3\n",
            "Threads                  \t24\n",
            "Min codons in orf        \t30\n",
            "Max codons in length     \t32734\n",
            "Max orf gaps             \t2147483647\n",
            "Contig start mode        \t2\n",
            "Contig end mode          \t2\n",
            "Orf start mode           \t1\n",
            "Forward frames           \t1,2,3\n",
            "Reverse frames           \t1,2,3\n",
            "Translation table        \t1\n",
            "Translate orf            \t0\n",
            "Use all table starts     \tfalse\n",
            "Offset of numeric ids    \t0\n",
            "Create lookup            \t0\n",
            "Compressed               \t0\n",
            "Add orf stop             \tfalse\n",
            "Overlap between sequences\t0\n",
            "Sequence split mode      \t1\n",
            "Strand selection         \t1\n",
            "Remove temporary files   \tfalse\n",
            "\n",
            "indexdb /home/animeshs/content/mmseqs2/genexa/genexaDB /home/animeshs/content/mmseqs2/genexa/genexaDB --seed-sub-mat nucl:nucleotide.out,aa:VTML80.out -k 0 --alph-size nucl:5,aa:21 --comp-bias-corr 1 --max-seq-len 65535 --max-seqs 300 --mask 1 --mask-lower-case 0 --spaced-kmer-mode 1 -s 7.5 --k-score 0 --check-compatible 0 --search-type 0 --split 0 --split-memory-limit 0 -v 3 --threads 24 \n",
            "\n",
            "Estimated memory consumption: 2G\n",
            "Write VERSION (0)\n",
            "Write META (1)\n",
            "Write SCOREMATRIX3MER (4)\n",
            "Write SCOREMATRIX2MER (3)\n",
            "Write SCOREMATRIXNAME (2)\n",
            "Write SPACEDPATTERN (23)\n",
            "Write DBR1INDEX (5)\n",
            "Write DBR1DATA (6)\n",
            "Write HDR1INDEX (18)\n",
            "Write HDR1DATA (19)\n",
            "Write GENERATOR (22)\n",
            "Index table: counting k-mers\n",
            "[=================================================================] 100.00% 468.35K 2s 688ms\n",
            "Index table: Masked residues: 931982\n",
            "Index table: fill\n",
            "[=================================================================] 100.00% 468.35K 4s 46ms\n",
            "Index statistics\n",
            "Entries:          155110361\n",
            "DB size:          1375 MB\n",
            "Avg k-mer size:   2.423599\n",
            "Top 10 k-mers\n",
            "    TSGGGV\t2599\n",
            "    RAARQG\t2269\n",
            "    TSGGGI\t1980\n",
            "    AVQQSL\t1945\n",
            "    RLTKGS\t1733\n",
            "    TTGGGV\t1443\n",
            "    LAAAQQ\t896\n",
            "    IARQGS\t831\n",
            "    ALREVV\t810\n",
            "    TSGGGT\t772\n",
            "Write ENTRIES (9)\n",
            "Write ENTRIESOFFSETS (10)\n",
            "Write SEQINDEXDATASIZE (15)\n",
            "Write SEQINDEXSEQOFFSET (16)\n",
            "Write SEQINDEXDATA (14)\n",
            "Write ENTRIESNUM (12)\n",
            "Write SEQCOUNT (13)\n",
            "Time for merging to genexaDB.idx: 0h 0m 0s 5ms\n",
            "Time for processing: 0h 0m 17s 454ms\n",
            "mkdir: cannot create directory ‘/home/animeshs/content/mmseqs2/swissprot’: File exists\n",
            "\u001b[33m/home/animeshs/content/mmseqs2/swissprot/swissprotDB exists and will be overwritten.\n",
            "\u001b[39mdatabases UniProtKB/Swiss-Prot /home/animeshs/content/mmseqs2/swissprot/swissprotDB /home/animeshs/content/mmseqs2/tmp \n",
            "\n",
            "MMseqs Version:              \tcc7d7da30ec779d6a2e886438f8295f59e2192f1\n",
            "Force restart with latest tmp\tfalse\n",
            "Remove temporary files       \tfalse\n",
            "Compressed                   \t0\n",
            "Threads                      \t24\n",
            "Verbosity                    \t3\n",
            "\n",
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ea6jrhpfF4y",
        "colab_type": "text"
      },
      "source": [
        "# Gene Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh274cXnWlva",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bd06b6c-8f51-49f1-d7de-f3b9da72969a"
      },
      "source": [
        "# @title Upload prokaryotic genomes as FASTA or gzipped FASTA.\n",
        "#from google.colab import files\n",
        "#genome_dict = files.upload()\n",
        "#genome_dict = {\"/mnt/f/viral/GCF_000911955.1_ViralProj215788_genomic.tax.fasta\" : \"/mnt/f/viral/GCF_000911955.1_ViralProj215788_genomic.tax.fasta\"}\n",
        "genome_dict = {\"/mnt/z/Pseudomonas/Aas-gDNA1-W2-PaE_S8_L001_R.contigs.fasta\": \"/mnt/z/Pseudomonas/Aas-gDNA1-W2-PaE_S8_L001_R.contigs.fasta\"}\n",
        "print(genome_dict.keys())\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['/mnt/z/Pseudomonas/Aas-gDNA1-W2-PaE_S8_L001_R.contigs.fasta'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC4llyrGWIuK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "a366ec81-bba7-40d8-dd43-f717ae2393f9"
      },
      "source": [
        "# @title Find genes\n",
        "\n",
        "def tokenize_aa_seq(aa_seq):\n",
        "    \"\"\" Convert amino acid letters to integers.\"\"\"\n",
        "    table = {\"L\": 1,\n",
        "             \"V\": 2,\n",
        "             \"I\": 3,\n",
        "             \"M\": 4,\n",
        "             \"C\": 5,\n",
        "             \"A\": 6,\n",
        "             \"G\": 7,\n",
        "             \"S\": 8,\n",
        "             \"T\": 9,\n",
        "             \"P\": 10,\n",
        "             \"F\": 11,\n",
        "             \"Y\": 12,\n",
        "             \"W\": 13,\n",
        "             \"E\": 14,\n",
        "             \"D\": 15,\n",
        "             \"N\": 16,\n",
        "             \"Q\": 17,\n",
        "             \"K\": 18,\n",
        "             \"R\": 19,\n",
        "             \"H\": 20,\n",
        "             \"*\": 0,\n",
        "             \"X\": 0}\n",
        "    tokenized = torch.tensor([table[aa] for aa in aa_seq])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def get_start_codon(seq, orfcoords, strand):\n",
        "    if strand == 1:\n",
        "        # forward strand\n",
        "        startcoord = orfcoords[0]\n",
        "        return seq[startcoord-3:startcoord]\n",
        "    else:\n",
        "        # reverse strand\n",
        "        startcoord = orfcoords[1]\n",
        "        return seq[startcoord:startcoord+3].reverse_complement()\n",
        "\n",
        "\n",
        "def find_ORFs(nuc_seq, minimum_length):\n",
        "    \"\"\"find positions of all open reading frames in given reading frame\"\"\"\n",
        "    if translation_table == 11:\n",
        "        starts = set([\"ATG\", \"GTG\", \"TTG\"])\n",
        "        stops = set([\"TAA\", \"TAG\", \"TGA\"])\n",
        "    elif translation_table == 4:\n",
        "        starts = set([\"ATG\", \"GTG\", \"TTG\"])\n",
        "        stops = set([\"TAA\", \"TAG\"])\n",
        "    else:\n",
        "        print(\"Translation table \", translation_table, \" not implemented. Please open a GitHub issue if this is a problem.\")\n",
        "        sys.exit()\n",
        "\n",
        "    ORF_startstop = []\n",
        "    temp_starts = []\n",
        "    l = len(nuc_seq)\n",
        "    for i in range(0, l, 3): \n",
        "        if i==0 or nuc_seq[i:i+3] in starts:\n",
        "            temp_starts.append(i)\n",
        "            continue\n",
        "        if ((nuc_seq[i:i+3] in stops) or (i+3==l)) and len(temp_starts) != 0:\n",
        "            for start in temp_starts:\n",
        "                if (i-start >= minimum_length):\n",
        "                    ORF_startstop.append((start, i))\n",
        "            temp_starts = []\n",
        "    return ORF_startstop\n",
        "\n",
        "def get_ORF_info(seq_list):\n",
        "    ORF_seq = []\n",
        "    ORF_coord = []\n",
        "    ORF_nucseq = []\n",
        "    for i, seq in enumerate(seq_list[:]):\n",
        "        # frame 0: starts at 0\n",
        "        # frame 1: starts at 1\n",
        "        # frame 2: starts at 2\n",
        "        # frame r0: ends at 0, MAY NOT START AT THE LAST COORD DUE TO MULTIPLE OF 3 DIFFERENCES\n",
        "        # frame r1: ends at 1\n",
        "        # frame r2: ends at 2\n",
        "\n",
        "        seqstr = str(seq)\n",
        "        seq_c = seq.complement()\n",
        "        seqstr_c = str(seq_c)\n",
        "        l = len(seqstr)\n",
        "        frame_0_end = (l-0)-(l-0)%3+0\n",
        "        frame_1_end = (l-1)-(l-1)%3+1\n",
        "        frame_2_end = (l-2)-(l-2)%3+2\n",
        "\n",
        "        frame_0 = find_ORFs(seqstr[0:frame_0_end], min_orf_length)\n",
        "        frame_1 = find_ORFs(seqstr[1:frame_1_end], min_orf_length)\n",
        "        frame_2 = find_ORFs(seqstr[2:frame_2_end], min_orf_length)\n",
        "\n",
        "        frame_r0 = find_ORFs(seqstr_c[0:frame_0_end][::-1], min_orf_length)\n",
        "        frame_r1 = find_ORFs(seqstr_c[1:frame_1_end][::-1], min_orf_length)\n",
        "        frame_r2 = find_ORFs(seqstr_c[2:frame_2_end][::-1], min_orf_length)\n",
        "\n",
        "        # standardize coords\n",
        "        ORF_0f_standard_nuccoord = [(x[0]+3, x[1]) for x in frame_0]\n",
        "        ORF_1f_standard_nuccoord = [(x[0]+4, x[1]+1) for x in frame_1]\n",
        "        ORF_2f_standard_nuccoord = [(x[0]+5, x[1]+2) for x in frame_2]\n",
        "\n",
        "        ORF_0r_standard_nuccoord = [(frame_0_end-x[1], frame_0_end-x[0]-3) for x in frame_r0]\n",
        "        ORF_1r_standard_nuccoord = [(frame_1_end-x[1], frame_1_end-x[0]-3) for x in frame_r1]\n",
        "        ORF_2r_standard_nuccoord = [(frame_2_end-x[1], frame_2_end-x[0]-3) for x in frame_r2]\n",
        "\n",
        "        # translate once per frame, then slice\n",
        "        aa_0 = str(seq[0:frame_0_end].translate(table=translation_table, to_stop=False))\n",
        "        aa_1 = str(seq[1:frame_1_end].translate(table=translation_table, to_stop=False))\n",
        "        aa_2 = str(seq[2:frame_2_end].translate(table=translation_table, to_stop=False))\n",
        "        aa_r0 = str(seq_c[0:frame_0_end][::-1].translate(table=translation_table, to_stop=False))\n",
        "        aa_r1 = str(seq_c[1:frame_1_end][::-1].translate(table=translation_table, to_stop=False))\n",
        "        aa_r2 = str(seq_c[2:frame_2_end][::-1].translate(table=translation_table, to_stop=False))\n",
        "\n",
        "        ORF_0f_aa = [aa_0[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_0] # reversed because model is trained with first amino acid directly upstream of stop codon\n",
        "        ORF_1f_aa = [aa_1[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_1] \n",
        "        ORF_2f_aa = [aa_2[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_2]\n",
        "        ORF_0r_aa = [aa_r0[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_r0]\n",
        "        ORF_1r_aa = [aa_r1[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_r1]\n",
        "        ORF_2r_aa = [aa_r2[slice(*tuple(int(idx/3) for idx in x))][::-1] for x in frame_r2]\n",
        "\n",
        "        ORF_seq.append([ORF_0f_aa, ORF_1f_aa, ORF_2f_aa, \n",
        "                        ORF_0r_aa, ORF_1r_aa, ORF_2r_aa])\n",
        "        ORF_coord.append([ORF_0f_standard_nuccoord, ORF_1f_standard_nuccoord, ORF_2f_standard_nuccoord, \n",
        "                          ORF_0r_standard_nuccoord, ORF_1r_standard_nuccoord, ORF_2r_standard_nuccoord])\n",
        "        \n",
        "        ORF_nucseq.append([str(seq[0:frame_0_end]), # all 5' to 3'\n",
        "                           str(seq[1:frame_1_end]),\n",
        "                           str(seq[2:frame_2_end]),\n",
        "                           str(seq_c[0:frame_0_end][::-1]),\n",
        "                           str(seq_c[1:frame_1_end][::-1]),\n",
        "                           str(seq_c[2:frame_2_end][::-1])])\n",
        "    return ORF_seq, ORF_nucseq, ORF_coord\n",
        "\n",
        "\n",
        "def analyze_overlap(coords0, coords1, strand0, strand1,\n",
        "                    unidirectional_penalty_per_base,\n",
        "                    convergent_penalty_per_base,\n",
        "                    divergent_penalty_per_base):\n",
        "    overlap = coords0[1] - coords1[0] # TODO account for fully overlapped gene\n",
        "\n",
        "    if overlap <= 0:\n",
        "        compatible, penalty = True, 0\n",
        "        return compatible, penalty\n",
        "    \n",
        "    if overlap > max_gene_overlap:\n",
        "        compatible, penalty = False, 0\n",
        "        return compatible, penalty\n",
        "\n",
        "    # get prime locations\n",
        "    if strand0 == 1:\n",
        "        threeprime0 = coords0[1]\n",
        "        fiveprime0 = coords0[0]\n",
        "    else:\n",
        "        threeprime0 = coords0[0]\n",
        "        fiveprime0 = coords0[1]\n",
        "    if strand1 == 1:\n",
        "        threeprime1 = coords1[1]\n",
        "        fiveprime1 = coords1[0]\n",
        "    else:\n",
        "        threeprime1 = coords1[0]\n",
        "        fiveprime1 = coords1[1]\n",
        "    \n",
        "    # exclude ORFs in same frame sharing same stop codon\n",
        "    if strand0 == strand1 and threeprime0 == threeprime1:\n",
        "        compatible, penalty = False, 0\n",
        "        return compatible, penalty\n",
        "\n",
        "    # unidirectional overlap\n",
        "    if (threeprime0 < fiveprime0) == (threeprime1 < fiveprime1):\n",
        "        compatible, penalty = True, overlap * unidirectional_penalty_per_base\n",
        "        return compatible, penalty\n",
        "\n",
        "    # convergent overlap\n",
        "    if (fiveprime0 < threeprime1 <= threeprime0) or (fiveprime1 < threeprime0 <= threeprime1):\n",
        "        compatible, penalty = True, overlap * convergent_penalty_per_base\n",
        "        return compatible, penalty\n",
        "    \n",
        "    # divergent overlap\n",
        "    if (threeprime0 < fiveprime1 <= fiveprime0) or (threeprime1 < fiveprime0 <= fiveprime1):\n",
        "        compatible, penalty = True, overlap * divergent_penalty_per_base\n",
        "        return compatible, penalty\n",
        "\n",
        "    return True, 0 # edge case of exactly 1 ORF\n",
        "\n",
        "def predict(X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.device_count() > 0:\n",
        "            X_enc = F.one_hot(X, 21).permute(0,2,1).float().cuda()\n",
        "            probs = expit(model(X_enc).cpu())\n",
        "            del X_enc\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            X_enc = F.one_hot(X, 21).permute(0,2,1).float()\n",
        "            probs = expit(model(X_enc).cpu())\n",
        "\n",
        "    return probs\n",
        "\n",
        "def predict_tis(X):\n",
        "    model_tis.eval()\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.device_count() > 0:\n",
        "            X_enc = F.one_hot(X, 4).permute(0,2,1).float().cuda()\n",
        "        else:\n",
        "            X_enc = F.one_hot(X, 4).permute(0,2,1).float()\n",
        "        probs = expit(model_tis(X_enc).cpu())\n",
        "    return probs\n",
        "\n",
        "nuc_encode = {\"A\":0,\n",
        "              \"T\":1,\n",
        "              \"G\":2,\n",
        "              \"C\":3,\n",
        "              \"N\":0,\n",
        "              \"M\":0,\n",
        "              \"R\":0,\n",
        "              \"Y\":0,\n",
        "              \"W\":0,\n",
        "              \"K\":0}\n",
        "              \n",
        "start_enc = {\"ATG\":0,\n",
        "             \"GTG\":1,\n",
        "             \"TTG\":2}\n",
        "\n",
        "def tensor_to_seq(tensor):\n",
        "    table = {0: \"X\",\n",
        "             1: \"L\",\n",
        "             2: \"V\",\n",
        "             3: \"I\",\n",
        "             4: \"M\",\n",
        "             5: \"C\",\n",
        "             6: \"A\",\n",
        "             7: \"G\",\n",
        "             8: \"S\",\n",
        "             9: \"T\",\n",
        "             10: \"P\",\n",
        "             11: \"F\",\n",
        "             12: \"Y\",\n",
        "             13: \"W\",\n",
        "             14: \"E\",\n",
        "             15: \"D\",\n",
        "             16: \"N\",\n",
        "             17: \"Q\",\n",
        "             18: \"K\",\n",
        "             19: \"R\",\n",
        "             20: \"H\"}\n",
        "    return \"\".join([table[x] for x in tensor])\n",
        "\n",
        "def kmerfilter(seq):\n",
        "    kmerset = kmerize(seq, k_seengene)\n",
        "    s = [x in aa_kmer_set for x in kmerset]\n",
        "    seen = np.sum(s) >= multimer_threshold\n",
        "    return seen\n",
        "\n",
        "def kmerize(seq, k):\n",
        "    kmerset = set()\n",
        "    for i in range(len(seq) - k + 1):\n",
        "        kmer = tuple(seq[i: i + k].tolist())\n",
        "        kmerset.add(kmer)\n",
        "    return kmerset\n",
        "\n",
        "# find genes for each uploaded genome\n",
        "GCF_list = []\n",
        "contig_name_list = []\n",
        "contig_length_list = []\n",
        "contig_seq_list = []\n",
        "contig_gene_coord_list = []\n",
        "contig_gene_strand_list = []\n",
        "\n",
        "for genome_name in genome_dict.keys():\n",
        "    if verbose:\n",
        "        print(\"Reading fasta file\", str(genome_name) + \"...\\n\")\n",
        "\n",
        "    # read genome sequence\n",
        "    seq_list = []\n",
        "    contig_name_sublist = []\n",
        "    contig_length_sublist = []\n",
        "    if os.path.splitext(genome_name)[1].lower() == \".gz\":\n",
        "        with gzip.open(genome_name, \"rt\") as f:\n",
        "            for record in SeqIO.parse(f, \"fasta\"):\n",
        "                seq_list.append(record.seq)\n",
        "                contig_name_sublist.append(record.id)\n",
        "                contig_length_sublist.append(len(record.seq))\n",
        "    else:\n",
        "        with open(genome_name, \"rt\") as f:\n",
        "            for record in SeqIO.parse(f, \"fasta\"):\n",
        "                seq_list.append(record.seq)\n",
        "                contig_name_sublist.append(record.id)\n",
        "                contig_length_sublist.append(len(record.seq))\n",
        "    contig_name_list.append(contig_name_sublist)\n",
        "    contig_length_list.append(contig_length_sublist)\n",
        "    contig_seq_list.append(seq_list)\n",
        "\n",
        "    # get sequences and coordinates of ORFs\n",
        "    if verbose:\n",
        "        print(\"Finding and translating open reading frames...\\n\")\n",
        "\n",
        "    ORF_seq_list, ORF_nucseq_list, ORF_coord_list = get_ORF_info(seq_list)\n",
        "\n",
        "    # combine ORFs to submit to GPU in batches\n",
        "    ORF_seq_combine = []\n",
        "    for i, contig in enumerate(ORF_seq_list):\n",
        "        for j, frame in enumerate(contig):\n",
        "            for k, coord in enumerate(frame):\n",
        "                ORF_seq_combine.append(coord)\n",
        "\n",
        "    # encode amino acids as integers\n",
        "    if verbose:\n",
        "        print(\"Encoding amino acids...\\n\")\n",
        "    ORF_seq_enc = [tokenize_aa_seq(x) for x in ORF_seq_combine]\n",
        "\n",
        "    # seengene check\n",
        "    if protein_kmer_filter:\n",
        "        if verbose:\n",
        "            print(\"Applying protein kmer filter...\\n\")\n",
        "        seengene = []\n",
        "        for s in ORF_seq_enc:\n",
        "            kmerset = kmerize(s, k_seengene)\n",
        "            s = [x in aa_kmer_set for x in kmerset]\n",
        "            seen = np.sum(s) >= multimer_threshold\n",
        "\n",
        "            seengene.append(seen)\n",
        "\n",
        "    # score\n",
        "    if verbose:\n",
        "        print(\"Scoring ORFs with temporal convolutional network...\\n\")\n",
        "\n",
        "    # sort by length to minimize impact of batch padding \n",
        "    ORF_lengths = np.asarray([len(x) for x in ORF_seq_enc])\n",
        "    length_idx = np.argsort(ORF_lengths)\n",
        "    ORF_seq_sorted = [ORF_seq_enc[i] for i in length_idx]\n",
        "\n",
        "    # pad to allow creation of batch matrix\n",
        "    prob_list = []\n",
        "    for i in tqdm(range(0, len(ORF_seq_sorted), gene_batch_size), unit=\" batch\"):\n",
        "        batch = ORF_seq_sorted[i:i+gene_batch_size]\n",
        "        seq_lengths = torch.LongTensor(list(map(len, batch)))\n",
        "        seq_tensor = torch.zeros((len(batch), seq_lengths.max())).long()\n",
        "\n",
        "        for idx, (seq, seqlen) in enumerate(zip(batch, seq_lengths)):\n",
        "            seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "        pred_all = predict(seq_tensor)\n",
        "\n",
        "        pred = []\n",
        "        for j, length in enumerate(seq_lengths):\n",
        "            subseq = pred_all[j, 0, 0:int(length)]\n",
        "            predprob = float(expit(torch.mean(logit(subseq))))\n",
        "            pred.append(predprob)\n",
        "        \n",
        "        prob_list.extend(pred)\n",
        "    prob_arr = np.asarray(prob_list, dtype=float)\n",
        "\n",
        "    # unsort\n",
        "    unsort_idx = np.argsort(length_idx)\n",
        "    ORF_prob = prob_arr[unsort_idx]\n",
        "\n",
        "    # recombine ORFs\n",
        "    idx = 0\n",
        "    ORF_gene_score = copy.deepcopy(ORF_coord_list) # fill coord matrix with scores\n",
        "    for i, contig in enumerate(ORF_gene_score):\n",
        "        for j, frame in enumerate(contig):\n",
        "            for k, coord in enumerate(frame):\n",
        "                ORF_gene_score[i][j][k] = float(ORF_prob[idx])\n",
        "                idx += 1\n",
        "\n",
        "    # create strand information\n",
        "    ORF_strand_flat = []\n",
        "    for i, seq in enumerate(ORF_seq_list):\n",
        "        if not any(seq):\n",
        "            ORF_strand_flat.append([])\n",
        "            continue\n",
        "        n_forward = len(seq[0]) + len(seq[1]) + len(seq[2])\n",
        "        n_reverse = len(seq[3]) + len(seq[4]) + len(seq[5])\n",
        "        ORF_allframe_strand = [1]*n_forward + [-1]*n_reverse\n",
        "        ORF_strand_flat.append(ORF_allframe_strand)\n",
        "\n",
        "    # flatten coords within contigs\n",
        "    ORF_coord_flat = [[item for sublist in x for item in sublist] for x in ORF_coord_list]\n",
        "\n",
        "    # get ORF lengths\n",
        "    ORF_length_flat = [[coords[1]-coords[0] for coords in x] for x in ORF_coord_flat]\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Scoring translation initiation sites...\\n\")\n",
        "\n",
        "    # extract nucleotide sequence surrounding potential start codons\n",
        "    ORF_TIS_seq = copy.deepcopy(ORF_coord_list)\n",
        "    ORF_start_codon = copy.deepcopy(ORF_coord_list)\n",
        "\n",
        "    for i, contig in enumerate(ORF_TIS_seq):\n",
        "        n = 0 # count to index into flat structure # TODO make sure this works as expected\n",
        "\n",
        "        nucseq = ORF_nucseq_list[i][0] # easier to use coords relative to single nucseq\n",
        "        nucseq_c = ORF_nucseq_list[i][3][::-1]\n",
        "        contig_nuclength = len(nucseq)\n",
        "\n",
        "\n",
        "        for j, frame in enumerate(contig):\n",
        "            for k, temp in enumerate(frame):\n",
        "                if any(temp):\n",
        "                    coords = ORF_coord_list[i][j][k]\n",
        "                    strand = ORF_strand_flat[i][n]\n",
        "                    n += 1\n",
        "                    if strand == 1:\n",
        "                        fiveprime = coords[0]\n",
        "                        if fiveprime >= 16 + 3: # NOTE 16 HARD CODED HERE\n",
        "                            downstream = nucseq[fiveprime: fiveprime + 16]\n",
        "                            upstream = nucseq[fiveprime - 16 - 3: fiveprime - 3]\n",
        "                            start_codon = start_enc[nucseq[fiveprime-3: fiveprime]]\n",
        "                            TIS_seq = torch.tensor([nuc_encode[c] for c in (upstream + downstream)[::-1]], dtype=int) # model scores 3' to 5' direction\n",
        "                        else:\n",
        "                            TIS_seq = -1 # deal with gene fragments later\n",
        "                            start_codon = 2\n",
        "\n",
        "                        ORF_TIS_seq[i][j][k] = TIS_seq\n",
        "                        ORF_start_codon[i][j][k] = start_codon\n",
        "                        \n",
        "                    else: # reverse strand\n",
        "                        fiveprime = coords[1]\n",
        "                        if contig_nuclength - fiveprime + 3 >= 16 + 3: # NOTE 16 HARD CODED HERE\n",
        "                            downstream = nucseq_c[fiveprime - 16: fiveprime][::-1]\n",
        "                            upstream = nucseq_c[fiveprime + 3: fiveprime + 3 + 16][::-1]\n",
        "                            start_codon = start_enc[nucseq_c[fiveprime: fiveprime + 3][::-1]]\n",
        "                            TIS_seq = torch.tensor([nuc_encode[c] for c in (upstream + downstream)[::-1]], dtype=int) # model scores 3' to 5' direction\n",
        "                        else:\n",
        "                            TIS_seq = -1 # deal with gene fragments later\n",
        "                            start_codon = 2\n",
        "                            \n",
        "                        ORF_TIS_seq[i][j][k] = TIS_seq\n",
        "                        ORF_start_codon[i][j][k] = start_codon\n",
        "\n",
        "    # flatten TIS for batching\n",
        "    ORF_TIS_prob = copy.deepcopy(ORF_TIS_seq)\n",
        "\n",
        "    ORF_TIS_seq_flat = []\n",
        "    ORF_TIS_seq_idx = []\n",
        "    for i, contig in enumerate(ORF_TIS_seq):\n",
        "        for j, frame in enumerate(contig):\n",
        "            for k, seq in enumerate(frame):\n",
        "                if type(seq) == int: # fragment\n",
        "                    ORF_TIS_prob[i][j][k] = 0.5 # HOW BEST TO DEAL WITH FRAGMENT TIS?\n",
        "                elif len(seq) != 32:\n",
        "                    ORF_TIS_prob[i][j][k] = 0.5 \n",
        "                else:\n",
        "                    ORF_TIS_seq_flat.append(seq)\n",
        "                    ORF_TIS_seq_idx.append((i, j, k))\n",
        "\n",
        "    # batch score TIS\n",
        "    TIS_prob_list = []\n",
        "    for i in tqdm(range(0, len(ORF_TIS_seq_flat), TIS_batch_size), unit=\" batch\"):\n",
        "        batch = ORF_TIS_seq_flat[i:i+TIS_batch_size]\n",
        "        TIS_stacked = torch.stack(batch)\n",
        "        pred = predict_tis(TIS_stacked)\n",
        "\n",
        "        TIS_prob_list.extend(pred)\n",
        "    y_pred_TIS = np.asarray(TIS_prob_list, dtype=float)\n",
        "\n",
        "    # reindex batched scores\n",
        "    for i, prob in enumerate(y_pred_TIS):\n",
        "        idx = ORF_TIS_seq_idx[i]\n",
        "        ORF_TIS_prob[idx[0]][idx[1]][idx[2]] = float(prob)\n",
        "\n",
        "    # combine all info into single score for each ORF\n",
        "    if protein_kmer_filter:\n",
        "        ORF_score_flat = []\n",
        "        for i, contig in enumerate(ORF_gene_score):\n",
        "            if not any(contig):\n",
        "                ORF_score_flat.append([])\n",
        "                continue\n",
        "            temp = []\n",
        "            seengene_idx = 0\n",
        "            for j, frame in enumerate(contig):\n",
        "                for k, geneprob in enumerate(frame):\n",
        "                    length = ORF_coord_list[i][j][k][1] - ORF_coord_list[i][j][k][0] + 1 \n",
        "                    TIS_prob = ORF_TIS_prob[i][j][k]\n",
        "                    start_codon = ORF_start_codon[i][j][k]\n",
        "                    ATG = start_codon == 0\n",
        "                    GTG = start_codon == 1\n",
        "                    TTG = start_codon == 2\n",
        "\n",
        "                    combprob =   geneprob * weight_gene_prob \\\n",
        "                            + TIS_prob * weight_TIS_prob \\\n",
        "                            + ATG * weight_ATG \\\n",
        "                            + GTG * weight_TTG \\\n",
        "                            + TTG * weight_GTG\n",
        "                    maxprob = weight_gene_prob + weight_TIS_prob + max(weight_ATG, weight_TTG, weight_GTG)\n",
        "                    probthresh = score_threshold * maxprob\n",
        "                    score = (combprob - probthresh) * length  + 1e6*seengene[seengene_idx]\n",
        "                    seengene_idx += 1\n",
        "\n",
        "                    temp.append(score)\n",
        "            ORF_score_flat.append(temp)\n",
        "\n",
        "    else:\n",
        "        ORF_score_flat = []\n",
        "        for i, contig in enumerate(ORF_gene_score):\n",
        "            if not any(contig):\n",
        "                ORF_score_flat.append([])\n",
        "                continue\n",
        "            temp = []\n",
        "            for j, frame in enumerate(contig):\n",
        "                for k, geneprob in enumerate(frame):\n",
        "                    length = ORF_coord_list[i][j][k][1] - ORF_coord_list[i][j][k][0] + 1 \n",
        "                    TIS_prob = ORF_TIS_prob[i][j][k]\n",
        "                    start_codon = ORF_start_codon[i][j][k]\n",
        "                    ATG = start_codon == 0\n",
        "                    GTG = start_codon == 1\n",
        "                    TTG = start_codon == 2\n",
        "\n",
        "                    combprob =   geneprob * weight_gene_prob \\\n",
        "                            + TIS_prob * weight_TIS_prob \\\n",
        "                            + ATG * weight_ATG \\\n",
        "                            + GTG * weight_TTG \\\n",
        "                            + TTG * weight_GTG\n",
        "                    maxprob = weight_gene_prob + weight_TIS_prob + max(weight_ATG, weight_TTG, weight_GTG)\n",
        "                    probthresh = score_threshold * maxprob\n",
        "                    score = (combprob - probthresh) * length\n",
        "\n",
        "                    temp.append(score)\n",
        "            ORF_score_flat.append(temp)\n",
        "\n",
        "    # DAGs to maximize geneiness on each contig\n",
        "    contig_gene_coord = []\n",
        "    contig_gene_strand = []\n",
        "\n",
        "    for i, coords in enumerate(ORF_coord_flat):\n",
        "        if verbose:\n",
        "            print(\"Creating graph of contig \" + str(i) + \"...\\n\")\n",
        "\n",
        "        # sort coords, lengths, strands, and scores\n",
        "        startpos = np.array([x[0] for x in coords])\n",
        "        sortidx = list(np.argsort(startpos))\n",
        "\n",
        "        coords_sorted = [coords[j] for j in sortidx]\n",
        "\n",
        "        lengths = ORF_length_flat[i]\n",
        "        lengths_sorted = [lengths[j] for j in sortidx]\n",
        "\n",
        "        scores = ORF_score_flat[i]\n",
        "        scores_sorted = [scores[j] for j in sortidx]\n",
        "\n",
        "        strands = ORF_strand_flat[i]\n",
        "        strands_sorted = [strands[j] for j in sortidx]\n",
        "\n",
        "        # create DAG\n",
        "        # keep track of graph path and score\n",
        "        predecessor = np.zeros(len(scores_sorted), dtype=np.int64)\n",
        "        max_path_score = np.zeros(len(scores_sorted), dtype=np.int64)\n",
        "\n",
        "        # add null starting node\n",
        "        n_connections = 0\n",
        "        idx_offset = 1\n",
        "        while n_connections < max_forward_connections:\n",
        "            k = idx_offset\n",
        "            idx_offset += 1\n",
        "            if k > len(scores_sorted)-1: # dont try to add edge past last ORF\n",
        "                n_connections += 1\n",
        "                continue\n",
        "            edge_weight = scores_sorted[k-1]\n",
        "\n",
        "            # initial scores from null node\n",
        "            max_path_score[k] = edge_weight\n",
        "            predecessor[k] = 0\n",
        "            \n",
        "            n_connections += 1\n",
        "\n",
        "        # add edges between compatible ORFs\n",
        "        for j in tqdm(range(1, len(scores_sorted)-1), unit=\" ORF\"):\n",
        "            n_connections = 0\n",
        "            idx_offset = 1\n",
        "\n",
        "            while n_connections < max_forward_connections:\n",
        "                k = j + idx_offset\n",
        "                idx_offset += 1\n",
        "\n",
        "                if k > len(scores_sorted)-1: # dont try to add edge past end of contigs\n",
        "                    n_connections += 1\n",
        "                    continue \n",
        "\n",
        "                coords0 = coords_sorted[j-1]\n",
        "                coords1 = coords_sorted[k-1]\n",
        "\n",
        "                strand0 = strands_sorted[j-1]\n",
        "                strand1 = strands_sorted[k-1]\n",
        "\n",
        "                compat, penalty = analyze_overlap(coords0, coords1, \n",
        "                                                  strand0, strand1,\n",
        "                                                  unidirectional_penalty_per_base,\n",
        "                                                  convergent_penalty_per_base,\n",
        "                                                  divergent_penalty_per_base)\n",
        "\n",
        "                if compat:\n",
        "                    score = scores_sorted[k-1] - penalty\n",
        "\n",
        "                    path_score = max_path_score[j] + score\n",
        "                    if path_score > max_path_score[k]:\n",
        "                        max_path_score[k] = path_score\n",
        "                        predecessor[k] = j\n",
        "\n",
        "                    n_connections += 1\n",
        "\n",
        "\n",
        "        # solve for geneiest path through contig\n",
        "        if verbose:\n",
        "            print(\"Maximizing geneiness...\")\n",
        "\n",
        "        pred_idx = np.argmax(max_path_score)\n",
        "        pred_path = []\n",
        "        while pred_idx > 0:\n",
        "            pred_path.append(pred_idx)\n",
        "            pred_idx = predecessor[pred_idx]\n",
        "\n",
        "        # max_ORF_PATH = [x-1 for x in max_ORF_PATH_withnull[1:]]\n",
        "        max_ORF_PATH = [x-1 for x in pred_path[:]] # 0 isnt added\n",
        "\n",
        "        gene_predict_coords = [coords_sorted[j] for j in max_ORF_PATH]\n",
        "        gene_predict_strand = [strands_sorted[j] for j in max_ORF_PATH]\n",
        "\n",
        "        # mmseqs filter\n",
        "        if mmseqs2_gene_filter:\n",
        "            if verbose:\n",
        "                print(\"\\nFiltering predictions with mmseqs2...\")\n",
        "\n",
        "            # get amino acid sequence from coherent ORFs\n",
        "            # 3' TO 5'\n",
        "            aa_sorted = [ORF_seq_enc[j] for j in sortidx]\n",
        "            aa_tensor = [aa_sorted[j] for j in max_ORF_PATH]\n",
        "            aa_seq = [tensor_to_seq([int(y) for y in x]) for x in aa_tensor]\n",
        "\n",
        "            # make temp dir to store mmseqs stuff\n",
        "            finding_empty_dir = True\n",
        "            dir_idx = 0\n",
        "            while finding_empty_dir:\n",
        "                dirpath = os.path.join(\"/home/animeshs/content/mmseqs2/tmp\", str(dir_idx))\n",
        "                if os.path.isdir(dirpath):\n",
        "                    dir_idx += 1\n",
        "                    continue\n",
        "                else:\n",
        "                    !mkdir {dirpath}\n",
        "                    finding_empty_dir = False\n",
        "            \n",
        "            # mmseqs create query DB     3' to 5'\n",
        "            query_fasta_path_35 = os.path.join(dirpath, \"candidate_genes_35.fasta\")\n",
        "            with open(query_fasta_path_35, \"w\") as f:\n",
        "                for i, s in enumerate(aa_seq):\n",
        "                    f.writelines(\">\" + str(i) + \"\\n\")\n",
        "                    f.writelines(str(s) + \"\\n\")\n",
        "\n",
        "            # mmseqs create query DB     5' to 3'\n",
        "            query_fasta_path_53 = os.path.join(dirpath, \"candidate_genes_53.fasta\")\n",
        "            with open(query_fasta_path_53, \"w\") as f:\n",
        "                for i, s in enumerate(aa_seq):\n",
        "                    f.writelines(\">\" + str(i) + \"\\n\")\n",
        "                    f.writelines(str(s)[::-1] + \"\\n\")\n",
        "\n",
        "            query_DB_path_35 = os.path.join(dirpath, \"candidateDB_35\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs createdb {query_fasta_path_35} {query_DB_path_35}\n",
        "            \n",
        "            query_DB_path_53 = os.path.join(dirpath, \"candidateDB_53\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs createdb {query_fasta_path_53} {query_DB_path_53}\n",
        "            \n",
        "            # mmseqs search\n",
        "            results_DB_path_35 = os.path.join(dirpath, \"resultsDB_35\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs search -s 7.0 {query_DB_path_35} {genexa_DB_path} {results_DB_path_35} {dirpath}\n",
        "\n",
        "            # convert to readable format\n",
        "            m8_path_genexa = os.path.join(dirpath, \"resultDB_genexa.m8\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs convertalis {query_DB_path_35} {genexa_DB_path} {results_DB_path_35} {m8_path_genexa} --format-output \"query,target,evalue,raw\"\n",
        "\n",
        "            # load search results\n",
        "            mmseqs_results_genexa = pandas.read_table(m8_path_genexa, header=None, names=[\"query\", \"target\", \"evalue\", \"raw\"]).to_numpy()\n",
        "\n",
        "            # get hits\n",
        "            hit_idx_genexa = np.unique(mmseqs_results_genexa[:, 0]).astype(int)\n",
        "\n",
        "            # mmseqs search\n",
        "            results_DB_path_53 = os.path.join(dirpath, \"resultsDB_53\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs search -s 7.0 {query_DB_path_53} {swissprot_DB_path} {results_DB_path_53} {dirpath}\n",
        "\n",
        "            # convert to readable format\n",
        "            m8_path_secondary = os.path.join(dirpath, \"resultDB_secondary.m8\")\n",
        "            !/home/animeshs/content/mmseqs2/mmseqs/bin/mmseqs convertalis {query_DB_path_53} {swissprot_DB_path} {results_DB_path_53} {m8_path_secondary} --format-output \"query,target,evalue,raw\"\n",
        "\n",
        "            # load search results\n",
        "            mmseqs_results_secondary = pandas.read_table(m8_path_secondary, header=None, names=[\"query\", \"target\", \"evalue\", \"raw\"]).to_numpy()\n",
        "\n",
        "            # get hits\n",
        "            hit_idx_secondary = np.unique(mmseqs_results_secondary[:, 0]).astype(int)\n",
        "\n",
        "            # filter gene predictions, keep if mmseqs hit or high enough gene score\n",
        "            cutoff = 200\n",
        "\n",
        "            cutoffpath = [x for i, x in enumerate(max_ORF_PATH) if (scores_sorted[x] > cutoff or (i in hit_idx_genexa or i in hit_idx_secondary))]\n",
        "            gene_predict_coords = [coords_sorted[j] for j in cutoffpath]\n",
        "            gene_predict_strand = [strands_sorted[j] for j in cutoffpath]\n",
        "\n",
        "            graph_score_cutoff = [scores_sorted[j] for j in cutoffpath]\n",
        "            contig_gene_coord.append(gene_predict_coords)\n",
        "            contig_gene_strand.append(gene_predict_strand)\n",
        "\n",
        "            n_genes = len(gene_predict_coords)\n",
        "            if verbose:\n",
        "                print(\"found\", n_genes, \"genes\\n\\n\")\n",
        "\n",
        "        else:\n",
        "            cutoff = 100\n",
        "            cutoffpath = [x for x in max_ORF_PATH if scores_sorted[x] > cutoff]\n",
        "            gene_predict_coords = [coords_sorted[j] for j in cutoffpath]\n",
        "            gene_predict_strand = [strands_sorted[j] for j in cutoffpath]\n",
        "\n",
        "            graph_score_cutoff = [scores_sorted[j] for j in cutoffpath]\n",
        "            contig_gene_coord.append(gene_predict_coords)\n",
        "            contig_gene_strand.append(gene_predict_strand)\n",
        "\n",
        "            n_genes = len(gene_predict_coords)\n",
        "            if verbose:\n",
        "                print(\"found\", n_genes, \"genes\\n\\n\")\n",
        "    contig_gene_coord_list.append(contig_gene_coord)\n",
        "    contig_gene_strand_list.append(contig_gene_strand)\n",
        "vira\n",
        "print(\"Done\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading fasta file /mnt/z/Pseudomonas/Aas-gDNA1-W2-PaE_S8_L001_R.contigs.fasta...\n",
            "\n",
            "Finding and translating open reading frames...\n",
            "\n",
            "Encoding amino acids...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-cdad90c4d565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoding amino acids...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mORF_seq_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_aa_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mORF_seq_combine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# seengene check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cdad90c4d565>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoding amino acids...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mORF_seq_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_aa_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mORF_seq_combine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# seengene check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cdad90c4d565>\u001b[0m in \u001b[0;36mtokenize_aa_seq\u001b[0;34m(aa_seq)\u001b[0m\n\u001b[1;32m     25\u001b[0m              \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m              \"X\": 0}\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cdad90c4d565>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m              \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m              \"X\": 0}\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'B'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3_sRPUzDfkX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title Download genome annotation (you may need to rerun this cell and/or allow multiple downloads in browser)\n",
        "# TODO support different output formats\n",
        "\n",
        "def write_GFF(start, end, contig, strand, contig_name_all, contig_length_all, contig_seq_all, GFF_path_out):\n",
        "    # writes to same format as Prokka GFF\n",
        "    bases_per_line = 60\n",
        "    with open(GFF_path_out, \"wt\") as f:\n",
        "        # header\n",
        "        f.writelines([\"##gff-version 3\\n\"])\n",
        "\n",
        "        # contig names and lengths\n",
        "        datalines = [\" \".join([\"##sequence-region\", \n",
        "                               str(contig_name_all[i]), \n",
        "                               \"1\", \n",
        "                               str(contig_length_all[i]), \n",
        "                               \"\\n\"]) for i in range(len(contig_name_all))]\n",
        "        f.writelines(datalines)\n",
        "\n",
        "        # CDS features\n",
        "        datalines = [\"\\t\".join([contig[i],\n",
        "                                \"Balrog\",\n",
        "                                \"CDS\",\n",
        "                                str(int(start[i]) + 1),\n",
        "                                end[i],\n",
        "                                \".\", # TODO: replace . with actual gene score\n",
        "                                strand[i],\n",
        "                                \"0\",\n",
        "                                \"inference=ab initio prediction:Balrog;product=hypothetical protein\"\n",
        "                                \"\\n\"]) for i in range(len(start))]\n",
        "        f.writelines(datalines)\n",
        "\n",
        "        # # contig sequences\n",
        "        # f.writelines([\"##FASTA\", \"\\n\"])\n",
        "        # for i, name in enumerate(contig_name_all):\n",
        "        #     f.writelines([\">\", str(name), \"\\n\"])\n",
        "        #     bases_per_line = 60\n",
        "        #     seq = contig_seq_all[i]\n",
        "        #     datalines = [str(seq[j:j+bases_per_line])+\"\\n\" for j in range(0, len(seq), bases_per_line)]\n",
        "        #     f.writelines(datalines)\n",
        "\n",
        "fasta_names = list(genome_dict.keys())\n",
        "gff_names = [str(x) + \"__.gff\" for x in fasta_names] # simpler than removing all combinations of fasta and gz from the end\n",
        "\n",
        "for genome_idx, gff in enumerate(gff_names):\n",
        "    # combine all info for gene predictions\n",
        "    contig_gene_start_flat = []\n",
        "    contig_gene_end_flat = []\n",
        "    contig_gene_strand_flat = []\n",
        "    contig_gene_contig_flat = []\n",
        "    try:\n",
        "        for i, contig_gene_coord in enumerate(contig_gene_coord_list[genome_idx]): # TODO get rid of jenky nested lists\n",
        "            for k, coord in enumerate(contig_gene_coord):\n",
        "                start = str(coord[0] - 3)\n",
        "                end = str(coord[1] + 3)\n",
        "                strandnum = contig_gene_strand_list[genome_idx][i][k]\n",
        "                if strandnum == 1:\n",
        "                    strand = \"+\"\n",
        "                else:\n",
        "                    strand = \"-\"\n",
        "                contig = str(contig_name_list[genome_idx][i])\n",
        "\n",
        "                contig_gene_start_flat.append(start)\n",
        "                contig_gene_end_flat.append(end)\n",
        "                contig_gene_strand_flat.append(strand)\n",
        "                contig_gene_contig_flat.append(contig)\n",
        "\n",
        "        write_GFF(contig_gene_start_flat, contig_gene_end_flat, contig_gene_contig_flat, contig_gene_strand_flat, \n",
        "                contig_name_list[genome_idx], contig_length_list[genome_idx], contig_seq_list[genome_idx], gff)\n",
        "    except:\n",
        "        print(\"Could not generate \", gff)\n",
        "\n",
        "for gff in gff_names:\n",
        "    try:\n",
        "        files.download(gff)\n",
        "    except:\n",
        "        print(\"Could not download \", gff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D-TsN2GZTFj",
        "colab_type": "text"
      },
      "source": [
        "# MIT License\n",
        " Copyright (c) 2020 Markus J. Sommer & Steven L. Salzberg\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}